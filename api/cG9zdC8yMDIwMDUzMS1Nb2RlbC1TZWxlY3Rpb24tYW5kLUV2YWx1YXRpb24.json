{"title":"Basic Model Selection and Evaluation","date":"2020-05-31T04:45:00.000Z","thumbnail":"https://i.loli.net/2020/09/09/r8vytkAuwUETsJZ.jpg","slug":"20200531-Model-Selection-and-Evaluation","tags":["Model Selection and Evaluation"],"categories":["Machine Lerning"],"updated":"2020-09-09T09:14:44.323Z","content":"<h1 id=\"Basic-Model-Selection-and-Evaluation\">Basic Model Selection and Evaluation<a href=\"post/20200531-Model-Selection-and-Evaluation#Basic-Model-Selection-and-Evaluation\"></a></h1><h3 id=\"Overview\">Overview<a href=\"post/20200531-Model-Selection-and-Evaluation#Overview\"></a></h3><p>(概略图)</p>\n<p><strong>· materials:</strong></p>\n<p>  · Wikipedia</p>\n<p>  · Machine Learning </p>\n<p>The recommended approach to solving machine learning problems is to:</p>\n<p>  <strong>1)</strong> Start with a simple algorithm, implement it quickly, test it early on your cross validation data.</p>\n<p>  <strong>2)</strong> Diagnosing bias and variance by ploting learning curves($ J_ {CV}(\\Theta)$ and $ J_ {train}(\\Theta)$) and gain guidance as to choose the right and effective solution to improve its performance.</p>\n<p>  <strong>3)</strong> Tune the model hyperparameters.</p>\n<p>​      For example: Getting more training examples, Adding features, Decreasing λ…</p>\n<p>  <strong>4)</strong> Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.</p>\n<h2 id=\"Model-Evaluation\">Model Evaluation<a href=\"post/20200531-Model-Selection-and-Evaluation#Model-Evaluation\"></a></h2><p>During the process of assessing the learning algorithm’s performance, it is important to do error analysis using some error metrics.</p>\n<h3 id=\"1-Bias-Variance-Trade-Off\">1. Bias-Variance Trade-Off<a href=\"post/20200531-Model-Selection-and-Evaluation#1-Bias-Variance-Trade-Off\"></a></h3><p>The prediction error for most machine learning algorithm can be broken down into three parts:</p>\n<ul>\n<li>Bias Error</li>\n<li>Variance Error</li>\n<li>Irreducible Error (introduced from the chosen framing of the problem which cannot be reduced.)</li>\n</ul>\n<p><strong>· Bias</strong> are the simplifying assumptions made by a model to make the target function easier to learn.</p>\n<ul>\n<li><strong>Low Bias</strong>: Suggests less assumptions about the form of the target function.</li>\n<li><strong>High-Bias</strong>: Suggests more assumptions about the form of the target function.</li>\n</ul>\n<p><strong>· Variance</strong> is the amount that the estimate of the target function will change if different training data was used.</p>\n<ul>\n<li><strong>Low Variance</strong>: Suggests small changes to the estimate of the target function with changes to the training dataset, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables.</li>\n<li><strong>High Variance</strong>: Suggests large changes to the estimate of the target function with changes to the training dataset, meaning that the specifics of the training have influences the number and types of parameters used to characterize the mapping function.</li>\n</ul>\n<p>Examples of <strong>low-bias</strong> and <strong>high-variance</strong> machine learning algorithms include: Decision Trees, k-Nearest Neighbors and support Vector Machine.</p>\n<p>Examples of <strong>high-bias</strong> and <strong>low-variance</strong> machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.</p>\n<p><u><a href=\"https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/\" target=\"_blank\" rel=\"noopener\">More details from here</a></u>.</p>\n<p><strong>Underfitting</strong>, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. </p>\n<p><strong>Overfitting</strong>, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</p>\n<p>Options address the issue of overfitting:</p>\n<p>1) Reduce the number of features. (Manually or use model selection algorithm)</p>\n<p>2) Regularization. (Keep all the features, but reduce the magnitude of parameters $ \\theta_j$.)</p>\n<p>Reduces over-fitting by adding a penalty to the loss function.</p>\n<p>3) Dropout</p>\n<h4 id=\"Regularization\">Regularization<a href=\"post/20200531-Model-Selection-and-Evaluation#Regularization\"></a></h4><p><u><a href=\"https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/\" target=\"_blank\" rel=\"noopener\">More details from here</a></u>.</p>\n<p><strong>1.For linear regression:</strong></p>\n<p><strong>Cost function:</strong></p>\n<p> $  J(\\theta) =  \\frac{1}{2m} [ \\sum^{m}_ {i=1} {( h_ \\theta(x^{(i)}) - y^{(i)} )^2} +  \\lambda \\sum^{n}_ {j=1} \\theta_j^2 ] $</p>\n<p>$ \\lambda $ is the regularization parameter. It determines how much the costs of our theta parameters are inflated. (we don’t penalize the $\\theta_0 $)</p>\n<p>Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting.</p>\n<p><strong>Gradient descent:</strong> </p>\n<p>$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_ {i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} $</p>\n<p>$ \\theta_j := \\theta_j (1-\\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_ {i=1}^m ( h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $</p>\n<p><strong>Normal Equation:</strong></p>\n<p>$\\theta = \\left( X^TX + \\lambda \\cdot L \\right)^{-1} X^Ty $</p>\n<p>$ \\text{where}\\ \\ L = \\begin{bmatrix} 0 &amp; &amp; &amp; &amp; \\newline &amp; 1 &amp; &amp; &amp; \\newline &amp; &amp; 1 &amp; &amp; \\newline &amp; &amp; &amp; \\ddots &amp; \\newline &amp; &amp; &amp; &amp; 1 \\newline\\end{bmatrix} $</p>\n<p>If m &lt; n (features), then $ X^TX $ is non-invertible. However, when we add the term λ⋅L, then $X^TX + λ⋅L $ becomes invertible.</p>\n<p><strong>2.For logistic regression:</strong></p>\n<p><strong>Cost function:</strong></p>\n<p>$ J(\\theta) = - \\dfrac{1}{m} \\sum_ {i=1}^m[y^{(i)} \\log(h_ \\theta(x^{(i)})) + (1- y^{(i)}) \\log(1-h_ \\theta(x^{(i)})) ] + \\frac{1}{2m} \\lambda \\sum^{n}_ {j=1} \\theta_j^2  $</p>\n<p><strong>Gradient descent:</strong> </p>\n<p>$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_ {i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} $</p>\n<p>$ \\theta_j := \\theta_j (1-\\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_ {i=1}^m ( h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $</p>\n<h3 id=\"2-Precision-and-Recall-Trade-Off\">2. Precision and Recall Trade-Off<a href=\"post/20200531-Model-Selection-and-Evaluation#2-Precision-and-Recall-Trade-Off\"></a></h3><div class=\"article-bounded\"><div class=\"article-table\"><table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th>Actual positive</th>\n<th>Actual negative</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>Predicted positive</strong></td>\n<td>True Positive</td>\n<td>False Positive</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Predicted negative</strong></td>\n<td>False Negative</td>\n<td>True Negative</td>\n</tr>\n</tbody></table></div></div>\n<p>$ Accuracy  = \\frac{TP +TN }{TP + FP + FP + TN} $</p>\n<p>Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations and it is a great measure only when you have symmetric datasets and  false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, we have to breakdown the accuracy formula even further and find a better evaluation metric.</p>\n<p>$ Precision  = \\frac{TP}{TP + FP}$</p>\n<p>High precision means less data is predicted positive and relates to high false negative and low false positive.</p>\n<p>$ Recall  = \\frac{TP}{TP + FN}$ (Sensitivity)</p>\n<p>High recall means more data is predicted positive and relates to high false positive and low false negative.</p>\n<p>$ F_1  = 2 * \\frac{Precision * Recall}{Precision + Recall} $</p>\n<p>F1 Score is the weighted average of precision and recall. Therefore, this score takes both false positives and false negatives into account. So we can evaluate the model based on  the F1 score.</p>\n<img src=\"https://i.loli.net/2020/05/31/ADnS1bRrB6pK5qc.png\" alt=\"precision\" style=\"zoom:80%;\">\n\n<p>(from wikipedia)</p>\n<h2 id=\"Model-Selection\">Model Selection<a href=\"post/20200531-Model-Selection-and-Evaluation#Model-Selection\"></a></h2><h3 id=\"Train-Test-and-Validation-Datasets\">Train, Test and Validation Datasets<a href=\"post/20200531-Model-Selection-and-Evaluation#Train-Test-and-Validation-Datasets\"></a></h3><ul>\n<li><strong>Training Dataset</strong>: The sample of data used to fit the parameters of model. (60%)</li>\n<li><strong>Validation Dataset</strong>: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration. (20%)</li>\n<li><strong>Test Dataset</strong>: The sample of data that has not been used prior, either for training the model or tuning the model parameters and only used to provide an unbiased evaluation of the skill of the final tuned model when comparing or selecting between final models. (20%)</li>\n</ul>\n<h3 id=\"Learning-curve\">Learning curve<a href=\"post/20200531-Model-Selection-and-Evaluation#Learning-curve\"></a></h3><p>When we test our simple model on validation dataset, we can plot a learning curve with different hyperparameters to help us to gain guidance as to choose the right and effective solution to improve its performance.</p>\n<p>For example: the size of training set.</p>\n<p><strong>1) Experiencing high bias:</strong></p>\n<img src=\"https://i.loli.net/2020/05/31/bz2sP7rSZlapHov.png\" alt=\"high bias\" style=\"zoom:150%;\">\n\n<p><strong>Low training set size</strong>: causes $ J_ {train}(\\Theta)$ to be low and $ J_ {CV}(\\Theta)$ to be high.</p>\n<p><strong>Large training set size</strong>: causes both $ J_ {CV}(\\Theta)$ and $ J_ {train}(\\Theta)$ to be high with $ J_ {CV}(\\Theta) ≈ J_ {train}(\\Theta) $</p>\n<p>If a learning algorithm is suffering from <strong>high bias</strong>, getting more training data will not help much.</p>\n<p><strong>2) Experiencing high variance:</strong></p>\n<img src=\"https://i.loli.net/2020/05/31/d5fATNm4KjUVSeC.png\" alt=\"high variance\" style=\"zoom:150%;\">\n\n<p><strong>Low training set size</strong>: $ J_ {train}(\\Theta)$ will be low and $ J_ {CV}(\\Theta)$ will be high.</p>\n<p><strong>Large training set size</strong>: $ J_ {train}(\\Theta)$ increases with training set size and $ J_ {CV}(\\Theta)$ continues to decrease without leveling off. Also,$ J_ {train}(\\Theta) &lt;  J_ {CV}(\\Theta)$ but the difference between them remains significant.</p>\n<p>If a learning algorithm is suffering from <strong>high variance</strong>, getting more training data is likely to help.</p>\n<p>After ploting the learning curve, we can tune the model hyperparameters targetedly.</p>\n<p>For example:</p>\n<ul>\n<li><p><strong>Getting more training examples:</strong> Fixes high variance</p>\n</li>\n<li><p><strong>Trying smaller sets of features:</strong> Fixes high variance</p>\n</li>\n<li><p><strong>Adding features:</strong> Fixes high bias</p>\n</li>\n<li><p><strong>Adding polynomial features:</strong> Fixes high bias</p>\n</li>\n<li><p><strong>Decreasing λ:</strong> Fixes high bias</p>\n</li>\n<li><p><strong>Increasing λ:</strong> Fixes high variance.</p>\n</li>\n</ul>\n","prev":{"title":"Support Vector Machines","slug":"20200605-Support-Vector-Machines"},"next":{"title":"Neural Networks","slug":"20200516-Neural-Networks"},"link":"https://oreodu.github.io/post/20200531-Model-Selection-and-Evaluation/","toc":[{"title":"Basic Model Selection and Evaluation","id":"Basic-Model-Selection-and-Evaluation","index":"1","children":[{"title":"Model Evaluation","id":"Model-Evaluation","index":"1.1","children":[{"title":"1. Bias-Variance Trade-Off","id":"1-Bias-Variance-Trade-Off","index":"1.1.1"},{"title":"2. Precision and Recall Trade-Off","id":"2-Precision-and-Recall-Trade-Off","index":"1.1.2"}]},{"title":"Model Selection","id":"Model-Selection","index":"1.2","children":[{"title":"Train, Test and Validation Datasets","id":"Train-Test-and-Validation-Datasets","index":"1.2.1"},{"title":"Learning curve","id":"Learning-curve","index":"1.2.2"}]}]}]}
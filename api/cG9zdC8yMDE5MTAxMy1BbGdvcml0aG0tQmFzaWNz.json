{"title":"Algorithm Basics","date":"2019-10-13T04:43:00.000Z","thumbnail":"https://i.loli.net/2020/09/09/xWpDLzIRHZ3ib71.jpg","slug":"20191013-Algorithm-Basics","tags":["Algorithms"],"categories":["Algorithms"],"updated":"2020-09-29T08:00:57.786Z","content":"<h2 id=\"Algorithm-Basics\">Algorithm Basics<a href=\"post/20191013-Algorithm-Basics#Algorithm-Basics\"></a></h2><h3 id=\"Overview\">Overview<a href=\"post/20191013-Algorithm-Basics#Overview\"></a></h3><p>(概略图)</p>\n<p><strong>· materials:</strong><br>  · <i>Algorithms</i>  by Dasgupta, C. H. Papadimitriou, U. V. Vazirani<br>  · <i>Efficient Algorithms and Intractable Problems</i>  (CS170 2018 Fall at UCB)<br>  · <i>Algorithm Design</i> by John Kleinberg, Eva Tardos<br>  · <a href=\"https://www.coursera.org/learn/algorithms-part1\" target=\"_blank\" rel=\"noopener\"><u>Algorithms, Part I,Ⅱ by Princeton University</u></a><br>  · Wikipedia</p>\n<h3 id=\"Part-1-What-is-Algorithm\">Part.1 What is Algorithm?<a href=\"post/20191013-Algorithm-Basics#Part-1-What-is-Algorithm\"></a></h3><p>The program is designed to solve certain problems and it mainly contains algorithms and data structures.</p>\n<blockquote>\n<p>An algorithm is a sequence of instructions. (from <i>Algorithm</i> Wikipedia)</p>\n</blockquote>\n<p>As usual, the algorithm use control structures (sequential, conditional, recurrent) and operators to make the instructions. A effective algorithms must have several characteristics as follows:<br><strong>1.Correctness</strong>: For each legal input, the algorithms must produce the expected output after a few precise and feasible instructions. (End within finite time.)<br><strong>2.Robustness</strong>: The algorithms must can apply in all possible circumstances that could arise and have the ability of tolerating perturbations and unexcepted exceptions.<br><strong>3.High efficiency and low storage requirement</strong></p>\n<h3 id=\"Part-2-How-to-design-a-proper-algorithm\">Part.2 How to design a proper algorithm?<a href=\"post/20191013-Algorithm-Basics#Part-2-How-to-design-a-proper-algorithm\"></a></h3><p>Typical steps in the development of algorithms:<br>1.Problem definition<br>2.Development of a model<br>3.Specification of the algorithm<br><strong>4</strong>.Designing an algorithm<br><strong>5</strong>.Checking the correctness of the algorithm<br><strong>6</strong>.Analysis of algorithm<br><strong>7</strong>.Implementation of algorithm<br>8.Program testing<br>9.Documentation preparation<br>(from <i>Algorithm</i> Wikipedia)</p>\n<p>When we finished design the algorithm, we should keep asking ourselves three questions as follows in order to make it more efficient.</p>\n<blockquote>\n<p>1.Is it correct (produce the expected output) ?  (<strong>Check</strong>)<br>2.How much time and storage does it take?  (<strong>Analyze</strong>)<br>3.Can we do better (higher efficiency and lower storage) ?  (<strong>optimize</strong>)<br>(from <i>Algorithms</i>  by Dasgupta, C. H. Papadimitriou, U. V. Vazirani)</p>\n</blockquote>\n<h3 id=\"Part-3-Classification\">Part.3 Classification<a href=\"post/20191013-Algorithm-Basics#Part-3-Classification\"></a></h3><p> There are various ways to classify algorithms.</p>\n<p><strong>By design paradigm</strong>:<br>· Brute-force or exhaustive search (try every possible solution)<br>· Search and enumeration<br>· Back tracking<br>· Divide and conquer (repeatedly reduces an instance of a problem to one or more smaller instances  until the instances are small enough to solve easily)<br>· Randomized algorithm (make some choices randomly (or pseudo-randomly))<br>· Reduction of complexity (transform the difficult problem into a better-known problem that we have asymptotically optimal algorithms.)</p>\n<p><strong>By implementation</strong>:<br>· Recursion ( invokes itself repeatedly)<br>· Logical (logic + control)<br>· Serial (execute one algorithm at a time), parallel or distributed (several processors work on a problem at the same time)<br>· Deterministic (exact decision at every step of the algorithm) or non-deterministic (guess through the use of heuristics)<br>· Exact (reach an exact solution) or approximate (closer to the true solution)<br>· Quantum algorithm</p>\n<p><strong>Optimization problems</strong><br>· Linear programming (the constraints of the problem can be used directly in producing the optimal solutions.)<br>· Dynamic programming (the optimal solution to a problem can be constructed from optimal solutions to subproblems and overlapping subproblems)<br>· The greedy method (start with some given or modified solutions)<br>· The heuristic method (find a solution close to the optimal solution)</p>\n<h3 id=\"Part-4-How-to-analyze-an-algorithm\">Part.4 How to analyze an algorithm?<a href=\"post/20191013-Algorithm-Basics#Part-4-How-to-analyze-an-algorithm\"></a></h3><p>It is frequently important to know how much of a particular resource (such as <strong>time or storage</strong>) is theoretically required for a given algorithm.  (from <i>Algorithm</i> Wikipedia). By analyzing the time complexity and space complexity we are able to improve the algorithm in a clearer direction. </p>\n<h4 id=\"Ⅰ-Time-complexity\">Ⅰ. Time complexity<a href=\"post/20191013-Algorithm-Basics#Ⅰ-Time-complexity\"></a></h4><p>In theory, we quantify the time through counting the number of elementary operations performed by the algorithm and multiply it by the time spent on each operation, where an elementary operation takes a fixed amount of time. Thus the amount of time taken by the same algorithm differ by a constant factor which is depended on the performance of different computers.</p>\n<p>In most cases, in order to analyze the algorithms in a more efficient way we should simplify the way we estimate the time complexity. Because only the number of the operations depends on algorithms and input data, so we can ignore the constant time spent on each elementary operation and express the running time by counting the number of the operations, as a function of the size of the input. Further, as the input size goes to infinity, we can use the basic operation as a proxy for the running time and ignore the lower order terms.</p>\n<p>So it is commonly expressed using the <strong>asymptotic notation</strong>, which lets us only focus on the big pictures. Asymptotic notation characterizes functions according to their growth rates: different functions with the same growth rate may be represented using the same notation. </p>\n<p>Since an algorithm’s performance time may vary with different inputs of  the same size, we commonly use the <strong>worst-case time complexity</strong>, denoted as $T(n)$, which is defined as the maximum amount of time taken on any input of size n. </p>\n<p><strong>Asymptotic notation</strong>:</p>\n<p>Let <i>f(n)</i> and <i>g(n)</i> be functions from positive integers to positive reals and think of them as the running time of two algorithms on inputs of size n. </p>\n<p>1.$ f = O(g)$, if there is a constant $c&gt;0$ and $n_0$, such that $|f(n)| \\leq c · g(n)$ for all $n&gt;n_0$<br>Big O notation provides an upper bound on the growth rate of the function.  <i>f</i> grows no faster than <i>g</i>. </p>\n<p>2.$g = Ω(f)$ ( <i>f</i> grows no faster than <i>g</i> ) if there is a constant $c &gt; 0$, such that $g(n) \\geq c · f(n)$</p>\n<p>3.$f = Θ(g)$( <i>g</i> grows the same as <i>f</i> ) if there is a constant $c &gt; 0$, such that $g(n) = c · f(n)$</p>\n<p>If the function <i>f</i> can be written as a finite sum of other functions, then the fastest growing one determines the order of $f(n)$. Here are some rules that can simplify <i>f</i> to <i>O(g)</i> by omitting coefficients and lower order terms.</p>\n<p>· Multiplicative constants can be omitted.<br>· log a dominates log b, $n^a$ dominates $n^b$ and $a^n$ dominates $b^n$ if $a&gt;b$<br>· Any exponential dominates any polynomial<br>· Any polynomial dominates any logarithm</p>\n<p>· $f_1= O(g_1)$ and $f_2 = O(g_2)$, $f_1· f_2 = O(g_1· g_2))$<br>· $f_1 = O(g_1)$ and $f_2 = O(g_2)$, $f_1 + f_2 = O(max(g_1, g_2))$</p>\n<p>· Orders of common functions:<br>$O(1)$, $O(\\log(\\log(n)))$, $O(\\log(n))$, $O((\\log(n))^c)  (c&gt;1)$,<br>$O(n^c)  (0&lt;c&lt;1)$, $O(n)$, $O(n\\log (n)) = O(\\log(n!))$,<br>$O(n^2)$, $O(n^c)$, $O(c^n)  (c&gt;1)$, $O(n!)$</p>\n<img src=\"https://i.loli.net/2020/09/29/IUthjneKSuP4wWX.png\" alt=\"123\" style=\"zoom:50%;\">\n\n<p>Upper bound: a specific algorithms.<br>Lower bound: proof that no algorithm can do better.</p>\n<p>Optimal algorithm: Lower bound equals upper bound to within a constant factor.</p>\n<p>When we design an algorithm,we usually focus on lower the upper bound(discover a new algorithm) and raise the lower bound.</p>\n<h4 id=\"Ⅱ-Space-complexity\">Ⅱ. Space complexity<a href=\"post/20191013-Algorithm-Basics#Ⅱ-Space-complexity\"></a></h4><p>The space complexity of an algorithm is the amount of the memory required by an algorithm to execute a program and produce output as a function of the size of the input. Similar to time complexity, Space complexity is often expressed asymptotically. such as $O(1)$,$O(n)$, $O(n\\log(n))$, $O(n^\\alpha)$, $O(2^n)$, etc., where n is the input size in units of bits needed to represent the input.</p>\n<h3 id=\"Part-5-Computational-Theory\">Part.5 Computational Theory<a href=\"post/20191013-Algorithm-Basics#Part-5-Computational-Theory\"></a></h3>","prev":{"title":"Searching and Sorting","slug":"20200102-Searching-and-Sorting"},"next":{"title":"Python Basics 01","slug":"20191010-Python-Basics-01"},"link":"https://oreodu.github.io/post/20191013-Algorithm-Basics/","toc":[{"title":"Algorithm Basics","id":"Algorithm-Basics","index":"1","children":[{"title":"Overview","id":"Overview","index":"1.1"},{"title":"Part.1 What is Algorithm?","id":"Part-1-What-is-Algorithm","index":"1.2"},{"title":"Part.2 How to design a proper algorithm?","id":"Part-2-How-to-design-a-proper-algorithm","index":"1.3"},{"title":"Part.3 Classification","id":"Part-3-Classification","index":"1.4"},{"title":"Part.4 How to analyze an algorithm?","id":"Part-4-How-to-analyze-an-algorithm","index":"1.5","children":[{"title":"Ⅰ. Time complexity","id":"Ⅰ-Time-complexity","index":"1.5.1"},{"title":"Ⅱ. Space complexity","id":"Ⅱ-Space-complexity","index":"1.5.2"}]},{"title":"Part.5 Computational Theory","id":"Part-5-Computational-Theory","index":"1.6"}]}]}
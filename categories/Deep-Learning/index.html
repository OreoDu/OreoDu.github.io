<!DOCTYPE html>

<html lang="en" class="no-js" id="body">

	<!-- BEGIN HEAD -->
<head><meta name="generator" content="Hexo 3.9.0">

  
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  <title></title>

  

  <meta name="description" content>

  


  <link rel="shortcut icon" href="https://cdn.shopify.com/s/files/1/2980/5252/files/favicon_32x32.png?v=1521894486" type="image/png">

  

<!-- Open Graph data-->
<meta property="og:type" content="blog">
<meta property="og:title" content>
<meta property="og:site_name" content>
<meta property="og:url" content="https://oreodu.github.io/categories/Deep-Learning/index.html">
<meta property="og:description" content>





<!-- end Open Graph data-->
  
<link href="https://fonts.googleapis.com/css?family=Hind:300,400,500,600,700" rel="stylesheet" type="text/css">
  
<!-- GLOBAL MANDATORY STYLES -->
<link href="/vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet" type="text/css">
<link href="/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/vendor/animate.css/animate.css" rel="stylesheet">


<link rel="stylesheet" href="/sass/layout.css">
  



<link rel="stylesheet" href="/css/prism-base16-ateliersulphurpool.light.css" type="text/css"></head>
<!-- END HEAD -->

  <body>
		
		


<!--========== HEADER ==========-->
<header class="header navbar-fixed-top">
  <!-- Navbar -->
  <nav class="navbar" role="navigation">
      <div class="container">
          <!-- Brand and toggle get grouped for better mobile display -->
          <div class="menu-container js_nav-item">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
                  <span class="sr-only">Toggle navigation</span>
                  <span class="toggle-icon"></span>
              </button>

              <!-- Logo -->
              <div class="logo">
                <!-- En caso de una text -->
                
                    <a class="logo-wrap" href="/#body" style="text-decoration: none;" >
                        <img class="logo-img logo-img-main" src="img/logo.png" alt="Asentus Logo">
                        <img class="logo-img logo-img-active" src="img/logo-dark.png" alt="Asentus Logo">
                    </a>
                
              </div>
              <!-- End Logo -->
          </div>

          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse nav-collapse">
              <div class="menu-container">
                  <ul class="nav navbar-nav navbar-nav-right">
                      
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#body"
                            >
                            Home
                        </a>
                        </li>
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#about"
                            >
                            About
                        </a>
                        </li>
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#experience"
                            >
                            Experience
                        </a>
                        </li>
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#work"
                            >
                            Work
                        </a>
                        </li>
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#contact"
                            >
                            Contact
                        </a>
                        </li>
                      
                  </ul>
              </div>
          </div>
          <!-- End Navbar Collapse -->
      </div>
  </nav>
  <!-- Navbar -->

</header>
<!--========== END HEADER ==========-->

		
  
    <article id="post-20200625-Convolutional-Neural-Networks" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/25/20200625-Convolutional-Neural-Networks/" class="article-date">
  <time datetime="2020-06-25T15:36:00.000Z" itemprop="datePublished">2020-06-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/25/20200625-Convolutional-Neural-Networks/">Convolutional Neural Networks</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Convolutional-Neural-Networks-CNN"><a href="#Convolutional-Neural-Networks-CNN" class="headerlink" title="Convolutional Neural Networks (CNN)"></a>Convolutional Neural Networks (CNN)</h1><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>(概略图)</p>
<p><strong>· materials:</strong></p>
<p>  · Wikipedia</p>
<p>  · <u><a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" target="_blank" rel="noopener">Sources NO.1</a></u></p>
<p>  · <u><a href="https://www.analyticsvidhya.com/blog/2020/02/mathematics-behind-convolutional-neural-network/?utm_source=blog&utm_medium=cnn-vs-rnn-vs-mlp-analyzing-3-types-of-neural-networks-in-deep-learning" target="_blank" rel="noopener">Sources NO.2</a></u></p>
<p>  · <u><a href="https://towardsdatascience.com/convolution-neural-networks-a-beginners-guide-implementing-a-mnist-hand-written-digit-8aa60330d022" target="_blank" rel="noopener">Sources NO.3</a></u></p>
<p><img src="https://i.loli.net/2020/07/17/Ve2RsvIFM7WozLS.jpg" alt="m"></p>
<p>A Convolutional Neural Network is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various objects in the image and be able to differentiate one from the other. ConvNets have the ability to learn these characteristics.</p>
<p>The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area.</p>
<h2 id="Why-CNN"><a href="#Why-CNN" class="headerlink" title="Why CNN?"></a>Why CNN?</h2><p>A CNN, in specific, has one or more layers of convolution filters comparing to the multilayer perceptron. A convolution filter receives its input from multiple units from the previous layer which together create a proximity.</p>
<p>The convolution layers (as well as pooling layers) are especially beneficial as:</p>
<ul>
<li><strong>·</strong>They uset the parameter sharing. A single filter is applied across different parts of an input to produce a feature map. So, they reduce the number of parameters in the network which reduces the chance of overfitting as the model would be less complex than a fully connected network (MLP).  Also, different filters can extract different kinds of features from an input.</li>
<li><strong>·</strong> They consider the shared information (the spatial features) in the small neighborhoods. Filters in the ConNets are used to extract the relevant features from the input using the convolution operation.</li>
<li><strong>·</strong> They learn the filters automatically without mentioning it explicitly. These filters help in extracting the right and relevant features from the input data.</li>
</ul>
<h2 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h2><h3 id="Convolution-layer"><a href="#Convolution-layer" class="headerlink" title="Convolution layer"></a>Convolution layer</h3><p><img src="https://i.loli.net/2020/07/17/gxehjuNRdkQEmM5.gif" alt="k"></p>
<p>The convolution filters or kernels moves to the right with a certain Stride Value and  do the dot product till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride Value and repeats the process until the entire image is traversed.</p>
<p>The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. Each filter can extract different features.</p>
<h4 id="Pading"><a href="#Pading" class="headerlink" title="Pading"></a>Pading</h4><p>Padding is a technique to simply add zeros around the margin of the image to increase it’s dimension. Padding allows us to emphasize the border pixels and in order lose less information.</p>
<img src="https://i.loli.net/2020/07/17/btVT1sBFp9c8I5E.gif" alt="pading" style="zoom: 67%;">

<p>Same Padding: Dimensionality is remains the same.</p>
<p>Valid Padding: Dimensionality is increased.</p>
<p>Feature size = ((Image size + 2 * Padding size − Kernel size) / Stride)+1</p>
<h3 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h3><p>Pooling layer helps reduce the spatial size of the convolved features and also helps reduce over-fitting by providing an abstracted representation of them. It is a sample-based discretization process. Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.</p>
<p><img src="https://i.loli.net/2020/07/17/heRa2M5WCoPvilI.gif" alt="p"></p>
<p>The kernel take the max or average of the region from the input overlapped by the kernel. </p>
<p>Max Pooling also performs as a Noise Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction. On the other hand, Average Pooling simply performs dimensionality reduction as a noise suppressing mechanism. Hence, we can say that Max Pooling performs a lot better than Average Pooling.</p>
<h3 id="Fully-connected-layer"><a href="#Fully-connected-layer" class="headerlink" title="Fully connected layer"></a>Fully connected layer</h3><p>Adding a Fully-Connected layer is a cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer.</p>
<p>We flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique.</p>
<img src="https://i.loli.net/2020/07/17/m6rhp5qJojD9PFN.jpg" alt="f" style="zoom: 67%;">



<p>Usually, activation function and dropout layer are used between two consecutive fully connected layers to introduce non-linearity and reduce over-fitting respectively. At the last fully connected layer we choose the output size based on our application. </p>
<h3 id="Dropout-layer"><a href="#Dropout-layer" class="headerlink" title="Dropout layer"></a>Dropout layer</h3><p>Dropout is a regularization technique used to reduce over-fitting on neural networks. Usually, deep learning models use dropout on the fully connected layers, but is also possible to use dropout after the max-pooling layers, creating image noise augmentation.</p>
<p>Dropout randomly zeroes some of the connections of the input tensor with probability p using samples from a Bernoulli distribution.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://oreodu.github.io/2020/06/25/20200625-Convolutional-Neural-Networks/" data-id="ckfs7glke000lx8izs9qn1vnl" class="article-share-link">share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Convolutional-Neural-Networks/">Convolutional Neural Networks</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-20200516-Neural-Networks" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/16/20200516-Neural-Networks/" class="article-date">
  <time datetime="2020-05-16T07:45:00.000Z" itemprop="datePublished">2020-05-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/16/20200516-Neural-Networks/">Neural Networks</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>(概略图)</p>
<p><strong>· materials:</strong></p>
<p>  · Wikipedia</p>
<p>  · Machine Learning </p>
<h3 id="Why-Neural-Networks"><a href="#Why-Neural-Networks" class="headerlink" title="Why Neural Networks?"></a>Why Neural Networks?</h3><h4 id="·-Decision-Boundary"><a href="#·-Decision-Boundary" class="headerlink" title="· Decision Boundary"></a>· Decision Boundary</h4><p>Every Machine Learning algorithm learns the mapping from an input to output. </p>
<p>In the case of classification problems, a decision boundary helps us in determining whether a given data point belongs to a certain class. Traditional machine learning algoritms cannot learn decision boundaries for nonlinear data.  However, Neural Network is capable of learning any nonlinear function.</p>
<p>Also, those algorithms are not capable of learning all the functions.</p>
<h4 id="·-Feature-engineering"><a href="#·-Feature-engineering" class="headerlink" title="· Feature engineering"></a>· Feature engineering</h4><p>In machine learning, we have to do feature  extraction and feature selection before we train the model. Feature engineering is a key step in the model building process. However, in deep Learning, we can automate the process of feature engineering.</p>
<h2 id="Feed-Forward-Neural-network-Multilayer-Perceptron"><a href="#Feed-Forward-Neural-network-Multilayer-Perceptron" class="headerlink" title="Feed-Forward Neural network (Multilayer Perceptron)"></a>Feed-Forward Neural network (Multilayer Perceptron)</h2><img src="https://i.loli.net/2020/06/22/DvejB2L6YuKFCcO.png" alt="mlp" style="zoom: 50%;">



<h3 id="Cost-Function-regularized"><a href="#Cost-Function-regularized" class="headerlink" title="Cost Function (regularized)"></a>Cost Function (regularized)</h3><p><strong>·</strong> L = total number of layers in the network</p>
<p><strong>·</strong> $ s_l $ = number of units (not counting bias unit) in layer l</p>
<p><strong>·</strong> K = number of output units/classes</p>
<p>$ J(\Theta) = - \frac{1}{m} \sum_ {i=1}^m \sum_ {k=1}^K \left[y^{(i)}_ k \log ((h_ \Theta (x^{(i)}))_ k) + (1 - y^{(i)}_ k)\log (1 - (h_ \Theta(x^{(i)}))_ k)\right] + \frac{\lambda}{2m}\sum_ {l=1}^{L-1} \sum_ {i=1}^{s_l} \sum_ {j=1}^{s_ {l+1}} ( \Theta_{j,i}^{(l)})^2 $</p>
<h3 id="Backpropagation-Algorithm"><a href="#Backpropagation-Algorithm" class="headerlink" title="Backpropagation Algorithm"></a>Backpropagation Algorithm</h3><p><strong>·</strong> Given training set $ { (x^{(1)}, y^{(1)}) … (x^{(m)}, y^{(m)})} $</p>
<p><strong>·</strong> Set $ \Delta^{(l)}_{i,j} := 0$ for all (l,i,j)</p>
<p><strong>·</strong> For training example t =1 to m:</p>
<p>​     <strong>1)</strong> $ a^{(1)} = x $</p>
<p>​     <strong>2)</strong> Perform forward propagation to compute $a^{(l)}$ for l=2,3,…,L</p>
<p>​           $ z^{(l)} = \Theta^{(l-1)} a^{(l-1)} $</p>
<p>​           $ a^{(l)} = g(z^{(l)}) $</p>
<p>​     <strong>3)</strong> Using $y^{(t)}$, compute $\delta^{(L)} = a^{(L)} - y^{(t)} $</p>
<p>​     <strong>4)</strong> Compute $ \delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)} $ </p>
<p>using $\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})  .* g^{′}(z^{(l)}) ),  ( g^{′}(z^{(l)})=a^{(l)} .* (1−a^{(l)}) ) $</p>
<p>​     <strong>5)</strong> $\Delta^{(l)}_ {i,j} := \Delta^{(l)}_ {i,j} + a^{(l)}_ j \delta^{(l+1)}_ j $ or with vectorization, $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)} (a^{(l)})^T $</p>
<p>​     Hence we update our new $\Delta$ matrix. ($\frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} = D^{(l)}_ {i,j}$)</p>
<p>​     $ D^{(l)}_ {i,j} =  \frac{1}{m} \Delta^{(l)}_ {i,j}, $ if j = 0</p>
<p>​     $D^{(l)}_ {i,j} =  \frac{1}{m}(\Delta^{(l)}_ {i,j} + \lambda \Theta^{(l)}_ {i,j}),$ if j≠0</p>
<p>In the actual programming implementation, we can separate back propagation through layers. </p>
<p><u><a href="https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/common/layers.py" target="_blank" rel="noopener">See  more details</a></u></p>
<h3 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h3><p>Gradient checking will assure that our backpropagation works as intended. </p>
<p>We can approximate the derivative of our cost function with: ($ ϵ =10^{−4} $)</p>
<p>$ \frac{∂J(\Theta)}{∂\Theta_ {j}} = \frac{J(\Theta_1 ,…, \Theta_j + ϵ ,…, \Theta_n) - J(\Theta_1 ,…, \Theta_j - ϵ ,…, \Theta_n)}{2ϵ}  $</p>
<h3 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h3><p>Activation functions introduce non-linearity to the model which allows it to learn complex functional mappings between the inputs and response variables. There are quite a few different activation functions like sigmoid, tanh, RelU, Leaky RelU, etc.</p>
<p><u><a href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/" target="_blank" rel="noopener">See  more details</a></u></p>
<h4 id="Sigmoid-Function"><a href="#Sigmoid-Function" class="headerlink" title="Sigmoid Function:"></a>Sigmoid Function:</h4><p>$ h_\theta (x) = g(\theta^T x) = \dfrac{1}{1 + e^{- \theta^T x}} $</p>
<p>Avoid overflow: </p>
<p>$  \dfrac{1}{1 + e^{- \theta^T x}} = \frac{1}{2} (tan(\frac{x}{2}) + 1)$</p>
<p>$ tanx = \frac{e^x - e^{-x}}{e^x + e^{-x}}  =  \frac{e^{2x} - 1 }{e^{2x} + 1} = \frac{1 - e^{-2x} }{1 + e^{-2x} }$</p>
<h4 id="ReLU-Function（Rectified-Linear-Unit）"><a href="#ReLU-Function（Rectified-Linear-Unit）" class="headerlink" title="ReLU Function（Rectified Linear Unit）:"></a>ReLU Function（Rectified Linear Unit）:</h4><p>$  h(x)= x ( x &gt; 0 )  or  0   ( x \leq 0 )  $</p>
<h4 id="Softmax-Function"><a href="#Softmax-Function" class="headerlink" title="Softmax Function:"></a>Softmax Function:</h4><p>$ h(x)_ k = \frac{exp(a_k)}{\sum_ {i=1}^K exp(a_i)} $</p>
<p>Avoid overflow: (c can be the - max(a))</p>
<p>$ h(x)_ k = \frac{exp(a_k + c)}{\sum_ {i=1}^K exp(a_i + c)} $ </p>
<p>The activation Function of the output layer depend on the specific problems. </p>
<p>For example: Binary classification — sigmoid function, Multiple Classification — softmax function, Regression — identity function</p>
<h3 id="Optimization-Function"><a href="#Optimization-Function" class="headerlink" title="Optimization Function"></a>Optimization Function</h3><p><u><a href="https://towardsdatascience.com/why-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096" target="_blank" rel="noopener">See  more details</a></u></p>
<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent:"></a>Gradient Descent:</h4><p>$ \Theta^{(l)}_ {i,j} = \Theta^{(l)}_ {i,j} - \eta \frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} $</p>
<p><strong>Standard Gradient descent</strong> updates the <em>parameters</em> only after each epoch.</p>
<p><strong>Stochastic gradient descent</strong> updates the <em>parameters</em> for <em>each observation</em> which leads to more number of updates<em>.</em> </p>
<p><strong>Mini-batch Gradient descent</strong> updates the parameters for a finite number of observations.</p>
<h4 id="Momentum-based-Gradient-Descent"><a href="#Momentum-based-Gradient-Descent" class="headerlink" title="Momentum-based Gradient Descent"></a>Momentum-based Gradient Descent</h4><p>$ v^{(l)}_ {i,j}  =  \alpha v^{(l)}_ {i,j}  - \eta \frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} $</p>
<p>$ \Theta^{(l)}_ {i,j} = \Theta^{(l)}_ {i,j} + v^{(l)}_ {i,j}  $</p>
<p> Momentum-based gradient descent remembers the update $v$ at each iteration, and determines the next update as a linear combination of the gradient and the previous update. Unlike in stochastic gradient descent, it tends to keep traveling in the same direction, preventing oscillations. </p>
<p>(Extension: Nesterov accelerated Gradient Descent)</p>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>$ h^{(l)}_ {i,j}  = h^{(l)}_ {i,j} +  ( \frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} )^2 $</p>
<p>$ \Theta^{(l)}_ {i,j} = \Theta^{(l)}_ {i,j} + \eta \frac{1}{\sqrt{h^{(l)}_ {i,j} } + \epsilon}  \frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} $</p>
<p>It adopts the learning rate(η) based on the <strong>sparsity</strong> of features. So the parameters with small updates(sparse features) have high <em>learning rate</em> whereas the parameters with large  updates(dense features) have low <em>learning rate</em>. Therefore adagrad uses a different <em>learning rate</em> for each <em>parameter.</em></p>
<p>(Extension: RMSProp)</p>
<h4 id="Adam-Adaptive-Moment-Estimation"><a href="#Adam-Adaptive-Moment-Estimation" class="headerlink" title="Adam(Adaptive Moment Estimation)"></a>Adam(Adaptive Moment Estimation)</h4><p>Adam algorithm introduces the concept of adaptive momentum along with adaptive learning rate. Adam is a combined form of Momentum-based GD and RMSProp.</p>
<img src="https://i.loli.net/2020/06/25/3eEICUzJWN5xLQA.png" alt="adm" style="zoom:50%;">



<h3 id="Initialize-the-weights"><a href="#Initialize-the-weights" class="headerlink" title="Initialize the weights"></a>Initialize the weights</h3><p><u><a href="https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78" target="_blank" rel="noopener">See  more details</a></u></p>
<h4 id="Zero-initialization-and-Random-initialization"><a href="#Zero-initialization-and-Random-initialization" class="headerlink" title="Zero initialization and Random initialization"></a>Zero initialization and Random initialization</h4><p>In general practice biases are initialized with 0 and weights are initialized with random numbers.</p>
<p>Usually, we can not initialize weight with the same values, high values or very low values. Otherwize, weight uniformity will happen or the gradients may vanish or explode quickly.</p>
<h4 id="Xavier-initialization"><a href="#Xavier-initialization" class="headerlink" title="Xavier initialization"></a>Xavier initialization</h4><p>It is mostly used for tanh() or sigmoid() activation function.</p>
<img src="https://i.loli.net/2020/06/25/AKQsCMUT6bedVtw.png" alt="Xavier" style="zoom: 50%;">

<img src="https://i.loli.net/2020/06/25/6QfRzvADObthNVy.png" alt="Xavier1" style="zoom: 50%;">



<h4 id="He-initialization"><a href="#He-initialization" class="headerlink" title="He initialization"></a>He initialization</h4><p>It is mostly used for ReLU() activation function.</p>
<img src="https://i.loli.net/2020/06/25/x5W4gyvkenXT3zf.png" alt="he" style="zoom: 50%;">



<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>Apart from the input layer, we can also normalize the hiden layer by adjusting and scaling the input. Batch normalization reduces the amount by what the hidden unit values shift around (covariance shift) and allows each layer of a network to learn by itself a little bit more independently of other layers. Also, it reduces overfitting because it has a slight regularization effects. Similar to dropout, it adds some noise to each hidden layer’s activations.</p>
<p>See  more details <u><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">here</a></u> and <u><a href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c" target="_blank" rel="noopener">here</a></u></p>
<img src="https://i.loli.net/2020/06/25/IGgC3xaJDARne75.png" alt="BN" style="zoom:50%;">



<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p><u><a href="https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5" target="_blank" rel="noopener">More details from here</a></u>.</p>
<p>In order to prevent over-fitting, we can use dropout to ignore units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random so we can reduce interdependent learning amongst the neurons.</p>
<p><img src="https://i.loli.net/2020/06/25/hFOIB3paKbc1jmG.png" alt="drop"></p>
<p>(from the paper”Dropout: a simple way to prevent neural networks from overfitting”, JMLR 2014)</p>
<h3 id="Main-Steps"><a href="#Main-Steps" class="headerlink" title="Main Steps"></a>Main Steps</h3><p>First, pick a network architecture.</p>
<p>​    · Number of input units = dimension of features $x^{(i)}$</p>
<p>​    · Number of output units = number of classes</p>
<p>​    · Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)</p>
<p>Next, training a Neural Network.</p>
<p>​    · Randomly initialize the weights</p>
<p>​    · Implement forward propagation to get $ h_\Theta(x^{(i)}) $ for any $ x^{(i)} $</p>
<p>​    · Implement the cost function</p>
<p>​    · Implement backpropagation to compute partial derivatives</p>
<p>​    · Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.</p>
<p>​    · Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://oreodu.github.io/2020/05/16/20200516-Neural-Networks/" data-id="ckfs7glk2000bx8izvf384xdw" class="article-share-link">share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Neural-Networks/">Neural Networks</a></li></ul>

    </footer>
  </div>
  
</article>


  




		


<footer class="footer">
  <div class="content container">
      <!--// row -->
      <div class="row">
          <div class="col-xs-6">
            
              <img class="footer-logo" src="img/logo-dark.png" alt="">
            
          </div>
          <div class="col-xs-6 text-right sm-text-left">
              <p class="margin-b-0">
                <a class="fweight-700" href="https://hexo.io/">Hexo.io</a> adaptation by <a class="fweight-700" href="https://molavec.com">Molavec</a>.
                <br>
                <br>
                <a class="fweight-700" href="https://keenthemes.com/preview/aircv/">Aircv</a>. Theme Powered by: <a class="fweight-700" href="https://www.keenthemes.com/">KeenThemes.com</a>.
            </p>
          </div>
      </div>
      <!--// end row -->
  </div>
</footer>
		<!-- Back To Top -->
<a href="javascript:void(0);" class="js-back-to-top back-to-top">Top</a>

<!-- JAVASCRIPTS(Load javascripts at bottom, this will reduce page load time) -->
<!-- CORE PLUGINS -->
<script src="/vendor/jquery.min.js" type="text/javascript"></script>
<script src="/vendor/jquery-migrate.min.js" type="text/javascript"></script>
<script src="/vendor/bootstrap/js/bootstrap.min.js" type="text/javascript"></script>

<!-- PAGE LEVEL PLUGINS -->
<script src="/vendor/jquery.easing.js" type="text/javascript"></script>
<script src="/vendor/jquery.back-to-top.js" type="text/javascript"></script>
<script src="/vendor/jquery.wow.min.js" type="text/javascript"></script>
<script src="/vendor/jquery.parallax.min.js" type="text/javascript"></script>
<script src="/vendor/jquery.appear.js" type="text/javascript"></script>
<script src="/vendor/masonry/jquery.masonry.pkgd.min.js" type="text/javascript"></script>
<script src="/vendor/masonry/imagesloaded.pkgd.min.js" type="text/javascript"></script>

<!-- PAGE LEVEL SCRIPTS -->


<script src="/js/layout.js"></script>
<script src="/js/components/progress-bar.js"></script>
<script src="/js/components/masonry.js"></script>
<script src="/js/components/wow.js"></script>

		

  </body>
</html>
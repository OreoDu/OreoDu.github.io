<!DOCTYPE html>

<html lang="en" class="no-js" id="body">

	<!-- BEGIN HEAD -->
<head><meta name="generator" content="Hexo 3.9.0">

  
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  

  <title></title>

  

  <meta name="description" content>

  


  <link rel="shortcut icon" href="https://cdn.shopify.com/s/files/1/2980/5252/files/favicon_32x32.png?v=1521894486" type="image/png">

  

<!-- Open Graph data-->
<meta property="og:type" content="blog">
<meta property="og:title" content>
<meta property="og:site_name" content>
<meta property="og:url" content="https://oreodu.github.io/categories/Machine-Lerning/index.html">
<meta property="og:description" content>





<!-- end Open Graph data-->
  
<link href="https://fonts.googleapis.com/css?family=Hind:300,400,500,600,700" rel="stylesheet" type="text/css">
  
<!-- GLOBAL MANDATORY STYLES -->
<link href="/vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet" type="text/css">
<link href="/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/vendor/animate.css/animate.css" rel="stylesheet">


<link rel="stylesheet" href="/sass/layout.css">
  



<link rel="stylesheet" href="/css/prism-base16-ateliersulphurpool.light.css" type="text/css"></head>
<!-- END HEAD -->

  <body>
		
		


<!--========== HEADER ==========-->
<header class="header navbar-fixed-top">
  <!-- Navbar -->
  <nav class="navbar" role="navigation">
      <div class="container">
          <!-- Brand and toggle get grouped for better mobile display -->
          <div class="menu-container js_nav-item">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
                  <span class="sr-only">Toggle navigation</span>
                  <span class="toggle-icon"></span>
              </button>

              <!-- Logo -->
              <div class="logo">
                <!-- En caso de una text -->
                
                    <a class="logo-wrap" href="/#body" style="text-decoration: none;" >
                        <img class="logo-img logo-img-main" src="img/logo.png" alt="Asentus Logo">
                        <img class="logo-img logo-img-active" src="img/logo-dark.png" alt="Asentus Logo">
                    </a>
                
              </div>
              <!-- End Logo -->
          </div>

          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse nav-collapse">
              <div class="menu-container">
                  <ul class="nav navbar-nav navbar-nav-right">
                      
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#body"
                            >
                            Home
                        </a>
                        </li>
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#about"
                            >
                            About
                        </a>
                        </li>
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#experience"
                            >
                            Experience
                        </a>
                        </li>
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#work"
                            >
                            Work
                        </a>
                        </li>
                      
                        <li class="js_nav-item nav-item">
                            <a  class="nav-item-child nav-item-hover"
                                href="/#contact"
                            >
                            Contact
                        </a>
                        </li>
                      
                  </ul>
              </div>
          </div>
          <!-- End Navbar Collapse -->
      </div>
  </nav>
  <!-- Navbar -->

</header>
<!--========== END HEADER ==========-->

		
  
    <article id="post-20200716-Feature-Engineering" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/16/20200716-Feature-Engineering/" class="article-date">
  <time datetime="2020-07-16T09:32:00.000Z" itemprop="datePublished">2020-07-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Lerning/">Machine Lerning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/16/20200716-Feature-Engineering/">Feature Engineering</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>(概略图)</p>
<p><strong>· materials:</strong></p>
<p>  · Wikipedia</p>
<p>  · Machine Learning </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://oreodu.github.io/2020/07/16/20200716-Feature-Engineering/" data-id="ckfs7glki000nx8izo52of9y1" class="article-share-link">share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Feature-Engineering/">Feature Engineering</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-20200605-Support-Vector-Machines" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/05/20200605-Support-Vector-Machines/" class="article-date">
  <time datetime="2020-06-05T07:45:00.000Z" itemprop="datePublished">2020-06-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Lerning/">Machine Lerning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/06/05/20200605-Support-Vector-Machines/">Support Vector Machines</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h1><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>(概略图)</p>
<p><strong>· materials:</strong></p>
<p>  · Wikipedia</p>
<p>  · Machine Learning</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://oreodu.github.io/2020/06/05/20200605-Support-Vector-Machines/" data-id="ckfs7glk9000hx8izgnuxsxtu" class="article-share-link">share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Support-Vector-Machines/">Support Vector Machines</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-20200531-Model-Selection-and-Evaluation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/31/20200531-Model-Selection-and-Evaluation/" class="article-date">
  <time datetime="2020-05-31T04:45:00.000Z" itemprop="datePublished">2020-05-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Lerning/">Machine Lerning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/31/20200531-Model-Selection-and-Evaluation/">Basic Model Selection and Evaluation</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Basic-Model-Selection-and-Evaluation"><a href="#Basic-Model-Selection-and-Evaluation" class="headerlink" title="Basic Model Selection and Evaluation"></a>Basic Model Selection and Evaluation</h1><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>(概略图)</p>
<p><strong>· materials:</strong></p>
<p>  · Wikipedia</p>
<p>  · Machine Learning </p>
<p>The recommended approach to solving machine learning problems is to:</p>
<p>  <strong>1)</strong> Start with a simple algorithm, implement it quickly, test it early on your cross validation data.</p>
<p>  <strong>2)</strong> Diagnosing bias and variance by ploting learning curves($ J_ {CV}(\Theta)$ and $ J_ {train}(\Theta)$) and gain guidance as to choose the right and effective solution to improve its performance.</p>
<p>  <strong>3)</strong> Tune the model hyperparameters.</p>
<p>​      For example: Getting more training examples, Adding features, Decreasing λ…</p>
<p>  <strong>4)</strong> Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.</p>
<h2 id="Model-Evaluation"><a href="#Model-Evaluation" class="headerlink" title="Model Evaluation"></a>Model Evaluation</h2><p>During the process of assessing the learning algorithm’s performance, it is important to do error analysis using some error metrics.</p>
<h3 id="1-Bias-Variance-Trade-Off"><a href="#1-Bias-Variance-Trade-Off" class="headerlink" title="1. Bias-Variance Trade-Off"></a>1. Bias-Variance Trade-Off</h3><p>The prediction error for most machine learning algorithm can be broken down into three parts:</p>
<ul>
<li>Bias Error</li>
<li>Variance Error</li>
<li>Irreducible Error (introduced from the chosen framing of the problem which cannot be reduced.)</li>
</ul>
<p><strong>· Bias</strong> are the simplifying assumptions made by a model to make the target function easier to learn.</p>
<ul>
<li><strong>Low Bias</strong>: Suggests less assumptions about the form of the target function.</li>
<li><strong>High-Bias</strong>: Suggests more assumptions about the form of the target function.</li>
</ul>
<p><strong>· Variance</strong> is the amount that the estimate of the target function will change if different training data was used.</p>
<ul>
<li><strong>Low Variance</strong>: Suggests small changes to the estimate of the target function with changes to the training dataset, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables.</li>
<li><strong>High Variance</strong>: Suggests large changes to the estimate of the target function with changes to the training dataset, meaning that the specifics of the training have influences the number and types of parameters used to characterize the mapping function.</li>
</ul>
<p>Examples of <strong>low-bias</strong> and <strong>high-variance</strong> machine learning algorithms include: Decision Trees, k-Nearest Neighbors and support Vector Machine.</p>
<p>Examples of <strong>high-bias</strong> and <strong>low-variance</strong> machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.</p>
<p><u><a href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/" target="_blank" rel="noopener">More details from here</a></u>.</p>
<p><strong>Underfitting</strong>, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. </p>
<p><strong>Overfitting</strong>, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</p>
<p>Options address the issue of overfitting:</p>
<p>1) Reduce the number of features. (Manually or use model selection algorithm)</p>
<p>2) Regularization. (Keep all the features, but reduce the magnitude of parameters $ \theta_j$.)</p>
<p>Reduces over-fitting by adding a penalty to the loss function.</p>
<p>3) Dropout</p>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p><u><a href="https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/" target="_blank" rel="noopener">More details from here</a></u>.</p>
<p><strong>1.For linear regression:</strong></p>
<p><strong>Cost function:</strong></p>
<p> $  J(\theta) =  \frac{1}{2m} [ \sum^{m}_ {i=1} {( h_ \theta(x^{(i)}) - y^{(i)} )^2} +  \lambda \sum^{n}_ {j=1} \theta_j^2 ] $</p>
<p>$ \lambda $ is the regularization parameter. It determines how much the costs of our theta parameters are inflated. (we don’t penalize the $\theta_0 $)</p>
<p>Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting.</p>
<p><strong>Gradient descent:</strong> </p>
<p>$ \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_ {i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} $</p>
<p>$ \theta_j := \theta_j (1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum_ {i=1}^m ( h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $</p>
<p><strong>Normal Equation:</strong></p>
<p>$\theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty $</p>
<p>$ \text{where}\ \ L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \newline &amp; 1 &amp; &amp; &amp; \newline &amp; &amp; 1 &amp; &amp; \newline &amp; &amp; &amp; \ddots &amp; \newline &amp; &amp; &amp; &amp; 1 \newline\end{bmatrix} $</p>
<p>If m &lt; n (features), then $ X^TX $ is non-invertible. However, when we add the term λ⋅L, then $X^TX + λ⋅L $ becomes invertible.</p>
<p><strong>2.For logistic regression:</strong></p>
<p><strong>Cost function:</strong></p>
<p>$ J(\theta) = - \dfrac{1}{m} \sum_ {i=1}^m[y^{(i)} \log(h_ \theta(x^{(i)})) + (1- y^{(i)}) \log(1-h_ \theta(x^{(i)})) ] + \frac{1}{2m} \lambda \sum^{n}_ {j=1} \theta_j^2  $</p>
<p><strong>Gradient descent:</strong> </p>
<p>$ \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_ {i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} $</p>
<p>$ \theta_j := \theta_j (1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum_ {i=1}^m ( h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $</p>
<h3 id="2-Precision-and-Recall-Trade-Off"><a href="#2-Precision-and-Recall-Trade-Off" class="headerlink" title="2. Precision and Recall Trade-Off"></a>2. Precision and Recall Trade-Off</h3><table>
<thead>
<tr>
<th align="center"></th>
<th>Actual positive</th>
<th>Actual negative</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>Predicted positive</strong></td>
<td>True Positive</td>
<td>False Positive</td>
</tr>
<tr>
<td align="center"><strong>Predicted negative</strong></td>
<td>False Negative</td>
<td>True Negative</td>
</tr>
</tbody></table>
<p>$ Accuracy  = \frac{TP +TN }{TP + FP + FP + TN} $</p>
<p>Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations and it is a great measure only when you have symmetric datasets and  false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, we have to breakdown the accuracy formula even further and find a better evaluation metric.</p>
<p>$ Precision  = \frac{TP}{TP + FP}$</p>
<p>High precision means less data is predicted positive and relates to high false negative and low false positive.</p>
<p>$ Recall  = \frac{TP}{TP + FN}$ (Sensitivity)</p>
<p>High recall means more data is predicted positive and relates to high false positive and low false negative.</p>
<p>$ F_1  = 2 * \frac{Precision * Recall}{Precision + Recall} $</p>
<p>F1 Score is the weighted average of precision and recall. Therefore, this score takes both false positives and false negatives into account. So we can evaluate the model based on  the F1 score.</p>
<img src="https://i.loli.net/2020/05/31/ADnS1bRrB6pK5qc.png" alt="precision" style="zoom:80%;">

<p>(from wikipedia)</p>
<h2 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h2><h3 id="Train-Test-and-Validation-Datasets"><a href="#Train-Test-and-Validation-Datasets" class="headerlink" title="Train, Test and Validation Datasets"></a>Train, Test and Validation Datasets</h3><ul>
<li><strong>Training Dataset</strong>: The sample of data used to fit the parameters of model. (60%)</li>
<li><strong>Validation Dataset</strong>: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration. (20%)</li>
<li><strong>Test Dataset</strong>: The sample of data that has not been used prior, either for training the model or tuning the model parameters and only used to provide an unbiased evaluation of the skill of the final tuned model when comparing or selecting between final models. (20%)</li>
</ul>
<h3 id="Learning-curve"><a href="#Learning-curve" class="headerlink" title="Learning curve"></a>Learning curve</h3><p>When we test our simple model on validation dataset, we can plot a learning curve with different hyperparameters to help us to gain guidance as to choose the right and effective solution to improve its performance.</p>
<p>For example: the size of training set.</p>
<p><strong>1) Experiencing high bias:</strong></p>
<img src="https://i.loli.net/2020/05/31/bz2sP7rSZlapHov.png" alt="high bias" style="zoom:150%;">

<p><strong>Low training set size</strong>: causes $ J_ {train}(\Theta)$ to be low and $ J_ {CV}(\Theta)$ to be high.</p>
<p><strong>Large training set size</strong>: causes both $ J_ {CV}(\Theta)$ and $ J_ {train}(\Theta)$ to be high with $ J_ {CV}(\Theta) ≈ J_ {train}(\Theta) $</p>
<p>If a learning algorithm is suffering from <strong>high bias</strong>, getting more training data will not help much.</p>
<p><strong>2) Experiencing high variance:</strong></p>
<img src="https://i.loli.net/2020/05/31/d5fATNm4KjUVSeC.png" alt="high variance" style="zoom:150%;">

<p><strong>Low training set size</strong>: $ J_ {train}(\Theta)$ will be low and $ J_ {CV}(\Theta)$ will be high.</p>
<p><strong>Large training set size</strong>: $ J_ {train}(\Theta)$ increases with training set size and $ J_ {CV}(\Theta)$ continues to decrease without leveling off. Also,$ J_ {train}(\Theta) &lt;  J_ {CV}(\Theta)$ but the difference between them remains significant.</p>
<p>If a learning algorithm is suffering from <strong>high variance</strong>, getting more training data is likely to help.</p>
<p>After ploting the learning curve, we can tune the model hyperparameters targetedly.</p>
<p>For example:</p>
<ul>
<li><p><strong>Getting more training examples:</strong> Fixes high variance</p>
</li>
<li><p><strong>Trying smaller sets of features:</strong> Fixes high variance</p>
</li>
<li><p><strong>Adding features:</strong> Fixes high bias</p>
</li>
<li><p><strong>Adding polynomial features:</strong> Fixes high bias</p>
</li>
<li><p><strong>Decreasing λ:</strong> Fixes high bias</p>
</li>
<li><p><strong>Increasing λ:</strong> Fixes high variance.</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://oreodu.github.io/2020/05/31/20200531-Model-Selection-and-Evaluation/" data-id="ckfs7glk5000fx8iz4vt2jguf" class="article-share-link">share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Model-Selection-and-Evaluation/">Model Selection and Evaluation</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-20200501-Machine-Learning-Basis" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/01/20200501-Machine-Learning-Basis/" class="article-date">
  <time datetime="2020-05-01T07:45:00.000Z" itemprop="datePublished">2020-05-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Lerning/">Machine Lerning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/01/20200501-Machine-Learning-Basis/">Basis &amp; Regression &amp; Classification</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Basis-amp-Regression-amp-Classification"><a href="#Basis-amp-Regression-amp-Classification" class="headerlink" title="Basis &amp; Regression &amp; Classification"></a>Basis &amp; Regression &amp; Classification</h1><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>(概略图)</p>
<p><strong>· materials:</strong></p>
<p>  · Wikipedia</p>
<p>  · Machine Learning </p>
<h2 id="1-Intro"><a href="#1-Intro" class="headerlink" title="1. Intro"></a>1. Intro</h2><p>Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”</p>
<h4 id="·-Supervised-Learning"><a href="#·-Supervised-Learning" class="headerlink" title="· Supervised Learning"></a>· Supervised Learning</h4><p>In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.</p>
<p>Supervised learning problems are categorized into <strong>“regression”</strong> and **”classification” ** problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.</p>
<h4 id="·-Unsupervised-Learning"><a href="#·-Unsupervised-Learning" class="headerlink" title="· Unsupervised Learning"></a>· Unsupervised Learning</h4><p>Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.</p>
<p>We can derive this structure by <strong>clustering</strong> the data based on relationships among the variables in the data.</p>
<p>With unsupervised learning there is no feedback based on the prediction results.</p>
<h4 id="·-Model"><a href="#·-Model" class="headerlink" title="· Model"></a>· Model</h4><p>To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn the hypothesis function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. </p>
<p>Inout : X</p>
<p>Output: Y</p>
<p>Hypothesis function: h</p>
<p>Cost function : J</p>
<p>Objective function: minimize (J)</p>
<h2 id="2-Regression"><a href="#2-Regression" class="headerlink" title="2. Regression"></a>2. Regression</h2><h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>$ h_\theta (x) =  \theta_1 x + \theta_0 $</p>
<h4 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h4><p>We can measure the accuracy of our hypothesis function by using a <strong>cost function</strong>. </p>
<p><strong>Mean Squared Error</strong></p>
<p>$  J(\theta_0, \theta_1) =  \frac{1}{2m} \sum^{m}_ {i=1} {( h_ \theta(x^{(i)}) - y^{(i)} )^2} $</p>
<p>$ J(\theta_0, \theta_1) $ can be ploted by a contour figure.</p>
<h4 id="Gradient-descent-minizining-the-cost-function-J"><a href="#Gradient-descent-minizining-the-cost-function-J" class="headerlink" title="Gradient descent (minizining the cost function J)"></a>Gradient descent (minizining the cost function J)</h4><p>$ \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} $</p>
<p>(Update $\theta_j$ simultaneously)</p>
<p>learning rate: <strong>α</strong></p>
<p>we should adjust our parameter <em>α</em> to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p>
<p>The method looks at every example in the entire training set on every step, and is called <strong>batch gradient descent</strong></p>
<p><u><a href="https://medium.com/@rohitpandey576/why-does-gradient-descent-work-128713588136" target="_blank" rel="noopener">Why does gradient descent work?</a></u></p>
<h3 id="Multivariate-Linear-Regression"><a href="#Multivariate-Linear-Regression" class="headerlink" title="Multivariate Linear Regression"></a>Multivariate Linear Regression</h3><p>Linear regression with multiple variables.</p>
<p>$ h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n $ </p>
<p>$ h_\theta(x) =\begin{bmatrix}\theta_0 \hspace{2em} \theta_1 \hspace{2em} … \hspace{2em} \theta_n\end{bmatrix}\begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix}= \theta^T X $</p>
<p>$ x_{0}^{(i)} =1 \text{ for } (i\in { 1,\dots, m } ) $</p>
<p>$  J(\theta) =  \frac{1}{2m} \sum^{m}_ {i=1} {( h_ \theta(x^{(i)}) - y^{(i)} )^2} $</p>
<h4 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h4><p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p>
<p>Two techniques to help with this are <strong>feature scaling</strong> and <strong>mean normalization</strong>.</p>
<p>feature scaling : $ x_i = \frac{x_i}{s_i} $</p>
<p>mean normalization : $ x_i = \frac{x_i - \mu_i}{s_i} $ </p>
<p>($ s_i $:  standard deviation , $ \mu_i $: average)</p>
<h4 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h4><p>$ \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} $</p>
<p><strong>Debugging gradient descent.</strong> </p>
<p>Make a plot with <em>number of iterations</em> and cost function J(θ).  It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration. </p>
<p>If <em>α</em> is too small: slow convergence.</p>
<p>If <em>α</em> is too large: may not decrease on every iteration and thus may not converge.</p>
<p>(Try like this: … 0.001 … 0.01 … 0.1 … 1)</p>
<h4 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h4><p>Normal Equation is a second way of minimizing J.</p>
<p> In the “Normal Equation” method, we will minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. This allows us to find the optimum theta without iteration. </p>
<p>$ \theta =( X_T  X)^{-1}X_TY  $</p>
<table>
<thead>
<tr>
<th>Gradient Descent</th>
<th><strong>Normal Equation</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Need to choose alpha</td>
<td>No need to choose alpha</td>
</tr>
<tr>
<td>Needs many iterations</td>
<td>No need to iterate</td>
</tr>
<tr>
<td>$ O(kn^2)$</td>
<td>$ O(n^3)$, need to calculate inverse of $ X^TX $</td>
</tr>
<tr>
<td>Works well when n is large</td>
<td>Slow if n is very large</td>
</tr>
</tbody></table>
<p>If $ X^TX $ is <strong>noninvertible,</strong> the common causes might be having :</p>
<p><strong>·</strong> Redundant features, where two features are very closely related (i.e. they are linearly dependent)<br><strong>·</strong> Too many features (e.g. m ≤ n). In this case, delete some features or use “regularization”.</p>
<h3 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h3><p>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</p>
<p>e.g.  $ h_\theta (x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 $</p>
<p>( if you choose your features this way then feature scaling becomes very important. $ 10^3 = 1000 $)</p>
<p>Feature choosing: We can <strong>combine</strong> multiple features into one. </p>
<h2 id="3-Classification-Logistic-regression"><a href="#3-Classification-Logistic-regression" class="headerlink" title="3. Classification(Logistic regression)"></a>3. Classification(Logistic regression)</h2><h3 id="Binary-classification"><a href="#Binary-classification" class="headerlink" title="Binary classification"></a>Binary classification</h3><h4 id="Activiation-Fuction"><a href="#Activiation-Fuction" class="headerlink" title="Activiation Fuction"></a>Activiation Fuction</h4><p>The activation Function of the output layer depend on the specific problems. </p>
<p>For example: Binary classification — sigmoid function, Multiple Classification — softmax function, Regression — identity function</p>
<p><strong>· Sigmoid Function</strong> (maps any real number to the (0, 1) interval)</p>
<p>$ h_\theta (x) = g(\theta^T x) = \dfrac{1}{1 + e^{- \theta^T x}} $</p>
<p>$ h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) $</p>
<p><strong>Decision Boundary</strong> (The property of the Hypothesis function)</p>
<p>$ h_\theta(x) = g(\theta^T x) \geq 0.5     when \theta^T x \geq 0 $</p>
<p>Non-linear decision boundary : $ \theta^T x = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 $ </p>
<h4 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h4><p>We cannot use the same cost function that we use for linear regression because the Logistic Function will not be a convex function, causing many local optima.</p>
<p><strong>Cross Entropy Error</strong></p>
<p>$ J(\theta) = \dfrac{1}{m} \sum_ {i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) $</p>
<p>$ Cost (h_ \theta(x^{(i)}),y^{(i)}) = -y^{(i)} \log(h_ \theta(x^{(i)}))  - (1- y^{(i)}) \log(1-h_ \theta(x^{(i)}))  $</p>
<p>$ \mathrm{Cost}(h_\theta(x),y) = 0 \text{ ,if } h_\theta(x) = y $</p>
<p>$ \mathrm{Cost}(h_ \theta(x),y) \rightarrow \infty \text{ ,if } y=0 \mathrm{and}  h_\theta(x) \rightarrow 1 $</p>
<p>$ \mathrm{Cost}(h_ \theta(x),y) \rightarrow \infty \text{ ,if } y=1  \mathrm{and}  h_\theta(x) \rightarrow 0 $</p>
<p>When $ h_ \theta(x^{(i)}) $ become so small, the value of log will become negative infinity. So in order to avoid such situation, we can add a small value to the $ h_ \theta(x^{(i)}) $, usually it can be 1e-7.</p>
<h4 id="Gradient-descent-1"><a href="#Gradient-descent-1" class="headerlink" title="Gradient descent"></a>Gradient descent</h4><p>$ \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} $</p>
<p>  <strong>Advanced Optimization</strong> </p>
<p> “Conjugate gradient”, “BFGS”, and “L-BFGS” are more sophisticated, faster ways to optimize θ and also no need to manually pick $ \alpha $ which can be used instead of gradient descent. </p>
<h3 id="Multiple-Classification"><a href="#Multiple-Classification" class="headerlink" title="Multiple Classification"></a>Multiple Classification</h3><p><strong>One-vs-all</strong></p>
<p>$ h_\theta^{(i)}(x) = P(y = i | x ; \theta)   y \in \lbrace0, 1 … n\rbrace$</p>
<p>$ \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) ) $</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://oreodu.github.io/2020/05/01/20200501-Machine-Learning-Basis/" data-id="ckfs7gljy000ax8izpjw14v66" class="article-share-link">share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Lesarning-Basis/">Machine Lesarning Basis</a></li></ul>

    </footer>
  </div>
  
</article>


  




		


<footer class="footer">
  <div class="content container">
      <!--// row -->
      <div class="row">
          <div class="col-xs-6">
            
              <img class="footer-logo" src="img/logo-dark.png" alt="">
            
          </div>
          <div class="col-xs-6 text-right sm-text-left">
              <p class="margin-b-0">
                <a class="fweight-700" href="https://hexo.io/">Hexo.io</a> adaptation by <a class="fweight-700" href="https://molavec.com">Molavec</a>.
                <br>
                <br>
                <a class="fweight-700" href="https://keenthemes.com/preview/aircv/">Aircv</a>. Theme Powered by: <a class="fweight-700" href="https://www.keenthemes.com/">KeenThemes.com</a>.
            </p>
          </div>
      </div>
      <!--// end row -->
  </div>
</footer>
		<!-- Back To Top -->
<a href="javascript:void(0);" class="js-back-to-top back-to-top">Top</a>

<!-- JAVASCRIPTS(Load javascripts at bottom, this will reduce page load time) -->
<!-- CORE PLUGINS -->
<script src="/vendor/jquery.min.js" type="text/javascript"></script>
<script src="/vendor/jquery-migrate.min.js" type="text/javascript"></script>
<script src="/vendor/bootstrap/js/bootstrap.min.js" type="text/javascript"></script>

<!-- PAGE LEVEL PLUGINS -->
<script src="/vendor/jquery.easing.js" type="text/javascript"></script>
<script src="/vendor/jquery.back-to-top.js" type="text/javascript"></script>
<script src="/vendor/jquery.wow.min.js" type="text/javascript"></script>
<script src="/vendor/jquery.parallax.min.js" type="text/javascript"></script>
<script src="/vendor/jquery.appear.js" type="text/javascript"></script>
<script src="/vendor/masonry/jquery.masonry.pkgd.min.js" type="text/javascript"></script>
<script src="/vendor/masonry/imagesloaded.pkgd.min.js" type="text/javascript"></script>

<!-- PAGE LEVEL SCRIPTS -->


<script src="/js/layout.js"></script>
<script src="/js/components/progress-bar.js"></script>
<script src="/js/components/masonry.js"></script>
<script src="/js/components/wow.js"></script>

		

  </body>
</html>
<!DOCTYPE HTML>
<html>

<head><meta name="generator" content="Hexo 3.9.0">
	<link rel="bookmark" type="image/x-icon" href="/img/logo_miccall.jpg">
	<link rel="shortcut icon" href="/img/logo_miccall.jpg">
	
			    <title>
    
    </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <link rel="stylesheet" href="/css/mic_main.css">
    <link rel="stylesheet" href="/css/dropdownMenu.css">
    <meta name="keywords" content="oreodu">
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css">
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<link rel="stylesheet" href="/css/prism-base16-ateliersulphurpool.light.css" type="text/css"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css">
<link rel="stylesheet" href="/css/typo.css">
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">Hello World</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special">
            <ul class="menu links">
			<!-- Homepage  主页  --> 
			<li>
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">Classification</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/Algorithms/">Algorithms</a></li><li><a class="category-link" href="/categories/Data-Structure/">Data Structure</a></li><li><a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li><a class="category-link" href="/categories/Machine-Lerning/">Machine Lerning</a></li><li><a class="category-link" href="/categories/Programming/">Programming</a></li><li><a class="category-link" href="/categories/Start-with-me/">Start with me</a>
	                    </li></ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/OreoDu" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main">
            <div class="post_page_title_img" style="height: 25rem;background-image: url(http://q9c32mgkt.bkt.clouddn.com/static/images/sha-pur.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;">
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2>Neural Networks</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>(概略图)</p>
<p><strong>· materials:</strong></p>
<p>  · Wikipedia</p>
<p>  · Machine Learning </p>
<h3 id="Why-Neural-Networks"><a href="#Why-Neural-Networks" class="headerlink" title="Why Neural Networks?"></a>Why Neural Networks?</h3><h4 id="·-Decision-Boundary"><a href="#·-Decision-Boundary" class="headerlink" title="· Decision Boundary"></a>· Decision Boundary</h4><p>Every Machine Learning algorithm learns the mapping from an input to output. </p>
<p>In the case of classification problems, a decision boundary helps us in determining whether a given data point belongs to a certain class. Traditional machine learning algoritms cannot learn decision boundaries for nonlinear data.  However, Neural Network is capable of learning any nonlinear function.</p>
<p>Also, those algorithms are not capable of learning all the functions.</p>
<h4 id="·-Feature-engineering"><a href="#·-Feature-engineering" class="headerlink" title="· Feature engineering"></a>· Feature engineering</h4><p>In machine learning, we have to do feature  extraction and feature selection before we train the model. Feature engineering is a key step in the model building process. However, in deep Learning, we can automate the process of feature engineering.</p>
<h2 id="Feed-Forward-Neural-network-Multilayer-Perceptron"><a href="#Feed-Forward-Neural-network-Multilayer-Perceptron" class="headerlink" title="Feed-Forward Neural network (Multilayer Perceptron)"></a>Feed-Forward Neural network (Multilayer Perceptron)</h2><img src="https://i.loli.net/2020/06/22/DvejB2L6YuKFCcO.png" alt="mlp" style="zoom: 50%;">



<h3 id="Cost-Function-regularized"><a href="#Cost-Function-regularized" class="headerlink" title="Cost Function (regularized)"></a>Cost Function (regularized)</h3><p><strong>·</strong> L = total number of layers in the network</p>
<p><strong>·</strong> $ s_l $ = number of units (not counting bias unit) in layer l</p>
<p><strong>·</strong> K = number of output units/classes</p>
<p>$ J(\Theta) = - \frac{1}{m} \sum_ {i=1}^m \sum_ {k=1}^K \left[y^{(i)}_ k \log ((h_ \Theta (x^{(i)}))_ k) + (1 - y^{(i)}_ k)\log (1 - (h_ \Theta(x^{(i)}))_ k)\right] + \frac{\lambda}{2m}\sum_ {l=1}^{L-1} \sum_ {i=1}^{s_l} \sum_ {j=1}^{s_ {l+1}} ( \Theta_{j,i}^{(l)})^2 $</p>
<h3 id="Backpropagation-Algorithm"><a href="#Backpropagation-Algorithm" class="headerlink" title="Backpropagation Algorithm"></a>Backpropagation Algorithm</h3><p><strong>·</strong> Given training set $ { (x^{(1)}, y^{(1)}) … (x^{(m)}, y^{(m)})} $</p>
<p><strong>·</strong> Set $ \Delta^{(l)}_{i,j} := 0$ for all (l,i,j)</p>
<p><strong>·</strong> For training example t =1 to m:</p>
<p>​     <strong>1)</strong> $ a^{(1)} = x $</p>
<p>​     <strong>2)</strong> Perform forward propagation to compute $a^{(l)}$ for l=2,3,…,L</p>
<p>​           $ z^{(l)} = \Theta^{(l-1)} a^{(l-1)} $</p>
<p>​           $ a^{(l)} = g(z^{(l)}) $</p>
<p>​     <strong>3)</strong> Using $y^{(t)}$, compute $\delta^{(L)} = a^{(L)} - y^{(t)} $</p>
<p>​     <strong>4)</strong> Compute $ \delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)} $ </p>
<p>using $\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})  .* g^{′}(z^{(l)}) ),  ( g^{′}(z^{(l)})=a^{(l)} .* (1−a^{(l)}) ) $</p>
<p>​     <strong>5)</strong> $\Delta^{(l)}_ {i,j} := \Delta^{(l)}_ {i,j} + a^{(l)}_ j \delta^{(l+1)}_ j $ or with vectorization, $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)} (a^{(l)})^T $</p>
<p>​     Hence we update our new $\Delta$ matrix. ($\frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} = D^{(l)}_ {i,j}$)</p>
<p>​     $ D^{(l)}_ {i,j} =  \frac{1}{m} \Delta^{(l)}_ {i,j}, $ if j = 0</p>
<p>​     $D^{(l)}_ {i,j} =  \frac{1}{m}(\Delta^{(l)}_ {i,j} + \lambda \Theta^{(l)}_ {i,j}),$ if j≠0</p>
<p>In the actual programming implementation, we can separate back propagation through layers. </p>
<p><u><a href="https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/common/layers.py" target="_blank" rel="noopener">See  more details</a></u></p>
<h3 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h3><p>Gradient checking will assure that our backpropagation works as intended. </p>
<p>We can approximate the derivative of our cost function with: ($ ϵ =10^{−4} $)</p>
<p>$ \frac{∂J(\Theta)}{∂\Theta_ {j}} = \frac{J(\Theta_1 ,…, \Theta_j + ϵ ,…, \Theta_n) - J(\Theta_1 ,…, \Theta_j - ϵ ,…, \Theta_n)}{2ϵ}  $</p>
<h3 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h3><p>Activation functions introduce non-linearity to the model which allows it to learn complex functional mappings between the inputs and response variables. There are quite a few different activation functions like sigmoid, tanh, RelU, Leaky RelU, etc.</p>
<p><u><a href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/" target="_blank" rel="noopener">See  more details</a></u></p>
<h4 id="Sigmoid-Function"><a href="#Sigmoid-Function" class="headerlink" title="Sigmoid Function:"></a>Sigmoid Function:</h4><p>$ h_\theta (x) = g(\theta^T x) = \dfrac{1}{1 + e^{- \theta^T x}} $</p>
<p>Avoid overflow: </p>
<p>$  \dfrac{1}{1 + e^{- \theta^T x}} = \frac{1}{2} (tan(\frac{x}{2}) + 1)$</p>
<p>$ tanx = \frac{e^x - e^{-x}}{e^x + e^{-x}}  =  \frac{e^{2x} - 1 }{e^{2x} + 1} = \frac{1 - e^{-2x} }{1 + e^{-2x} }$</p>
<h4 id="ReLU-Function（Rectified-Linear-Unit）"><a href="#ReLU-Function（Rectified-Linear-Unit）" class="headerlink" title="ReLU Function（Rectified Linear Unit）:"></a>ReLU Function（Rectified Linear Unit）:</h4><p>$  h(x)= x ( x &gt; 0 )  or  0   ( x \leq 0 )  $</p>
<h4 id="Softmax-Function"><a href="#Softmax-Function" class="headerlink" title="Softmax Function:"></a>Softmax Function:</h4><p>$ h(x)_ k = \frac{exp(a_k)}{\sum_ {i=1}^K exp(a_i)} $</p>
<p>Avoid overflow: (c can be the - max(a))</p>
<p>$ h(x)_ k = \frac{exp(a_k + c)}{\sum_ {i=1}^K exp(a_i + c)} $ </p>
<p>The activation Function of the output layer depend on the specific problems. </p>
<p>For example: Binary classification — sigmoid function, Multiple Classification — softmax function, Regression — identity function</p>
<h3 id="Optimization-Function"><a href="#Optimization-Function" class="headerlink" title="Optimization Function"></a>Optimization Function</h3><p><u><a href="https://towardsdatascience.com/why-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096" target="_blank" rel="noopener">See  more details</a></u></p>
<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent:"></a>Gradient Descent:</h4><p>$ \Theta^{(l)}_ {i,j} = \Theta^{(l)}_ {i,j} - \eta \frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} $</p>
<p><strong>Standard Gradient descent</strong> updates the <em>parameters</em> only after each epoch.</p>
<p><strong>Stochastic gradient descent</strong> updates the <em>parameters</em> for <em>each observation</em> which leads to more number of updates<em>.</em> </p>
<p><strong>Mini-batch Gradient descent</strong> updates the parameters for a finite number of observations.</p>
<h4 id="Momentum-based-Gradient-Descent"><a href="#Momentum-based-Gradient-Descent" class="headerlink" title="Momentum-based Gradient Descent"></a>Momentum-based Gradient Descent</h4><p>$ v^{(l)}_ {i,j}  =  \alpha v^{(l)}_ {i,j}  - \eta \frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} $</p>
<p>$ \Theta^{(l)}_ {i,j} = \Theta^{(l)}_ {i,j} + v^{(l)}_ {i,j}  $</p>
<p> Momentum-based gradient descent remembers the update $v$ at each iteration, and determines the next update as a linear combination of the gradient and the previous update. Unlike in stochastic gradient descent, it tends to keep traveling in the same direction, preventing oscillations. </p>
<p>(Extension: Nesterov accelerated Gradient Descent)</p>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>$ h^{(l)}_ {i,j}  = h^{(l)}_ {i,j} +  ( \frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} )^2 $</p>
<p>$ \Theta^{(l)}_ {i,j} = \Theta^{(l)}_ {i,j} + \eta \frac{1}{\sqrt{h^{(l)}_ {i,j} } + \epsilon}  \frac{∂J(\Theta)}{∂\Theta^{(l)}_ {i,j}} $</p>
<p>It adopts the learning rate(η) based on the <strong>sparsity</strong> of features. So the parameters with small updates(sparse features) have high <em>learning rate</em> whereas the parameters with large  updates(dense features) have low <em>learning rate</em>. Therefore adagrad uses a different <em>learning rate</em> for each <em>parameter.</em></p>
<p>(Extension: RMSProp)</p>
<h4 id="Adam-Adaptive-Moment-Estimation"><a href="#Adam-Adaptive-Moment-Estimation" class="headerlink" title="Adam(Adaptive Moment Estimation)"></a>Adam(Adaptive Moment Estimation)</h4><p>Adam algorithm introduces the concept of adaptive momentum along with adaptive learning rate. Adam is a combined form of Momentum-based GD and RMSProp.</p>
<img src="https://i.loli.net/2020/06/25/3eEICUzJWN5xLQA.png" alt="adm" style="zoom:50%;">



<h3 id="Initialize-the-weights"><a href="#Initialize-the-weights" class="headerlink" title="Initialize the weights"></a>Initialize the weights</h3><p><u><a href="https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78" target="_blank" rel="noopener">See  more details</a></u></p>
<h4 id="Zero-initialization-and-Random-initialization"><a href="#Zero-initialization-and-Random-initialization" class="headerlink" title="Zero initialization and Random initialization"></a>Zero initialization and Random initialization</h4><p>In general practice biases are initialized with 0 and weights are initialized with random numbers.</p>
<p>Usually, we can not initialize weight with the same values, high values or very low values. Otherwize, weight uniformity will happen or the gradients may vanish or explode quickly.</p>
<h4 id="Xavier-initialization"><a href="#Xavier-initialization" class="headerlink" title="Xavier initialization"></a>Xavier initialization</h4><p>It is mostly used for tanh() or sigmoid() activation function.</p>
<img src="https://i.loli.net/2020/06/25/AKQsCMUT6bedVtw.png" alt="Xavier" style="zoom: 50%;">

<img src="https://i.loli.net/2020/06/25/6QfRzvADObthNVy.png" alt="Xavier1" style="zoom: 50%;">



<h4 id="He-initialization"><a href="#He-initialization" class="headerlink" title="He initialization"></a>He initialization</h4><p>It is mostly used for ReLU() activation function.</p>
<img src="https://i.loli.net/2020/06/25/x5W4gyvkenXT3zf.png" alt="he" style="zoom: 50%;">



<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>Apart from the input layer, we can also normalize the hiden layer by adjusting and scaling the input. Batch normalization reduces the amount by what the hidden unit values shift around (covariance shift) and allows each layer of a network to learn by itself a little bit more independently of other layers. Also, it reduces overfitting because it has a slight regularization effects. Similar to dropout, it adds some noise to each hidden layer’s activations.</p>
<p>See  more details <u><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">here</a></u> and <u><a href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c" target="_blank" rel="noopener">here</a></u></p>
<img src="https://i.loli.net/2020/06/25/IGgC3xaJDARne75.png" alt="BN" style="zoom:50%;">



<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p><u><a href="https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5" target="_blank" rel="noopener">More details from here</a></u>.</p>
<p>In order to prevent over-fitting, we can use dropout to ignore units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random so we can reduce interdependent learning amongst the neurons.</p>
<p><img src="https://i.loli.net/2020/06/25/hFOIB3paKbc1jmG.png" alt="drop"></p>
<p>(from the paper”Dropout: a simple way to prevent neural networks from overfitting”, JMLR 2014)</p>
<h3 id="Main-Steps"><a href="#Main-Steps" class="headerlink" title="Main Steps"></a>Main Steps</h3><p>First, pick a network architecture.</p>
<p>​    · Number of input units = dimension of features $x^{(i)}$</p>
<p>​    · Number of output units = number of classes</p>
<p>​    · Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)</p>
<p>Next, training a Neural Network.</p>
<p>​    · Randomly initialize the weights</p>
<p>​    · Implement forward propagation to get $ h_\Theta(x^{(i)}) $ for any $ x^{(i)} $</p>
<p>​    · Implement the cost function</p>
<p>​    · Implement backpropagation to compute partial derivatives</p>
<p>​    · Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.</p>
<p>​    · Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 valine -->
<div id="comment">
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#comment' ,
        notify: false,
        verify: false,
        app_id: 'pi6wgQQwDcdAXXPAYDhLTXAM-gzGzoHsz',
        app_key: 'apbjMMH4cLAjmeGJWDdGGE01',
        placeholder: 'Please leave your footprints~~',
        pageSize: '10',
        avatar: 'retro',
        avatar_cdn: ''
    });
</script>
</div>
<style>
   #comment{
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
